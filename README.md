âœ¨ ğŸ’– A thinker like an engineer, a creator like a product developer.<br />

ğŸ‘‹ Hi, Iâ€™m Kristina Livesay. <br />

This account at the moment contains work samples I have accumulated over a decade.<br />

ğŸ‘©â€ğŸ’» I am a software engineer and product owner from Wolfram Research and Curtiss-Wright, Scientech.<br />
âœŒï¸  I have taken a career break for the last few years, to grow my family, and during that time, I delved in artificial intelligence, theory of mind and child development. I found some very interesting observations through reading the works of Mahler, Fairbarn and Masterson that have applications in machine learning and game theory.<br /> <br />
ğŸ‘€ Iâ€™m interested in large-scale software development, product management, applications to machine learning (decision problems, computer vision etc) and robotics. <br />
ğŸŒ± Iâ€™m currently learning machine learning fundamentals (Triple S book, Dan Roth notes, Sutton etc.). <br />
ğŸ’ï¸ Iâ€™m looking to work in software engineering or product management. <br /><br />

ğŸ“« e-mail: kumbullakristina@gmail.com <br /><br />

Publications in progress: <br /> <br />

A Stability-Theoretic Framework for Human-AI Debate and Epistemic Judgment <br />
ğŸ“– Quote: AI safety increasingly demands robust alignment protocols between human and artificial agents. One such
approach is debate: structured adversarial interaction intended to surface epistemically sound arguments.
While prior work such as AI Safety via Debate [?] frames debate as a zero-sum game judged by a human
arbiter, we propose a fundamentally different theoretical formulation.
Our framework treats debate as a stability-seeking process embedded in an evolving opinion network.
Drawing from the energy-minimization model of Stable Configurations in Social Networks [?], agents update
their beliefs over time in response to both internal biases and external epistemic feedback. Crucially, our
model uses judges not to declare winners and losers, but to reward interpretation, logical discernment, and
constructive correction â€” epistemic virtues that promote convergence toward stable and justified belief
states. <br /> <br />


Reframing the Alignment Problem: Object Relations, the Human Condition, and the Future of AI <br />
ğŸ“– Quote: In Platoâ€™s Republic, the Allegory of the Cave describes prisoners chained in a cave, watching shadows
cast on a wall by unseen objects behind them. Mistaking shadows for reality, the prisoners remain
unaware of the true Formsâ€”the ultimate, unchanging truths beyond mere appearances.
Todayâ€™s AI systems are, in a profound sense, the "prisoners" of Platoâ€™s cave. They learn not from
direct experience of reality, but from shadowsâ€”data reflections of human behavior, language, and
thought. These data are shaped by the biases, fragmentations, and unresolved contradictions of
our society. As such, an AI trained solely on this data internalizes distorted frames, incapable of
distinguishing between surface-level correlations and the deeper structures of meaning.
This shadow-based learning raises a critical issue for alignment: how can AI discern the True,
the Good, and the Beautifulâ€”Platoâ€™s highest idealsâ€”when trained on inputs that reflect only
partial, and often pathological, representations of them?
The focus of this paper is to identify elements by which we may judge AI systems, not only
by their functional performance but by how well they reflect or approximate these three Platonic
principles.
Without these principles as anchors, AI risks becoming an engine of amplificationâ€”replicating
the distortions in its training data and embedding human shadow into code. If humanity is still
wrestling with its own alignment to the Good, AI must be trained not only on what is said or done,
but also with a structure that seeks to see beyond the shadows. <br /><br />




